{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import random\n",
    "from representations.base import BaseImageRepresentation\n",
    "from model.base import BaseModel\n",
    "from utils.loss import ContrastiveLoss\n",
    "\n",
    "from data.evaluate import EvaluateData\n",
    "from data.training import TrainingData\n",
    "\n",
    "from execution.evaluate import Evaluate\n",
    "from execution.trainining import Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniele/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:128: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "learning_rate = 0.0001\n",
    "epochs = 1\n",
    "batch_size = 64\n",
    "num_workers = 2\n",
    "num_runs = 100\n",
    "feat_dim = 128\n",
    "k_shot = 1\n",
    "model_name = \"Resnet18\"\n",
    "dataset_name = \"ufop\"\n",
    "image_representation = \"Skeleton-DML\"\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_method = BaseImageRepresentation.get_type(image_representation)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.Grayscale(num_output_channels=3),  # Converte para 3 canais\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "from utils.logs import Logs\n",
    "\n",
    "\n",
    "class Data(Dataset):\n",
    "\tdef __init__(self, dataset_name='ufop', image_method=None, transform=None):\n",
    "\t\tself.dataset_name = dataset_name\n",
    "\t\tself.dataset = self.get_dataset()\n",
    "\t\tself.signs = self.get_signs(self.dataset)\n",
    "\t\tself.dataframe = self.prepare_data(self.dataset)\n",
    "\t\tself.categories = list(self.dataframe[\"category\"].unique())\n",
    "\t\tself.persons = list(self.dataframe[\"person\"].unique())\n",
    "\t\tself.image_method = image_method\n",
    "\t\tself.transform = transform\n",
    "\t\tself.image_size = self.transform.transforms[0].size\n",
    "\n",
    "\n",
    "\tdef __getitem__(self, index):\n",
    "\t\tdataframe = self.dataframe.iloc[index]\n",
    "\t\tx, y, category, person = dataframe[\"x\"], dataframe[\"y\"], dataframe[\"category\"], dataframe[\"person\"]\n",
    "\n",
    "\t\timage = self.image_method().transform(x, y)\n",
    "\t\timage = Image.fromarray(np.uint8(image * 255)).convert('RGB')\n",
    "\n",
    "\t\tif self.transform:\n",
    "\t\t\timage = self.transform(image)\n",
    "\n",
    "\t\treturn image, torch.tensor(self.categories.index(category), dtype=torch.int64), torch.tensor(self.persons.index(person), dtype=torch.int64)\n",
    "\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.dataframe)\n",
    "\n",
    "\n",
    "\tdef get_dataset(self):\n",
    "\t\ttry:\n",
    "\t\t\tdataset_file = f\"libras_{self.dataset_name}_openpose.csv\"\n",
    "\t\t\tdataset_path = os.path.join(f\"datasets/{self.dataset_name}\", dataset_file)\n",
    "\t\t\treturn pd.read_csv(dataset_path, low_memory=True)\n",
    "\t\texcept FileNotFoundError as e:\n",
    "\t\t\tLogs(logging.ERROR, f\"Dataset {self.dataset_name} not found. Error: {e}\")\n",
    "\t\t\treturn None\n",
    "\n",
    "\n",
    "\n",
    "\tdef get_features(self):\n",
    "\t\t# total_image_size = self.image_size[0] * self.image_size[1] * 3  # Multiplicando por 3 para RGB\n",
    "\n",
    "\t\t_X, _y, _p = [], [], []\n",
    "\t\t# np.empty((0, total_image_size)), np.empty((0, )), np.empty((0, ))\n",
    "\n",
    "\t\tfor index in range(self.dataframe.shape[0]):\n",
    "\t\t\t_image, _label, _person = self[index]\n",
    "\n",
    "\t\t\t_image = _image.cpu().numpy()\n",
    "\t\t\t_label = _label.cpu().numpy()\n",
    "\t\t\t_person = _person.cpu().numpy()\n",
    "\n",
    "\t\t\t_X.append(_image)\n",
    "\t\t\t_y.append(_label)\n",
    "\t\t\t_p.append(_person)\n",
    "\n",
    "\t\t\t# X = np.append(X, [image], axis=0)\n",
    "\t\t\t# y = np.append(y, [label], axis=0)\n",
    "\t\t\t# p = np.append(p, [person], axis=0)\n",
    "\n",
    "\n",
    "\t\tX = np.stack(_X)\n",
    "\t\ty = np.stack(_y)\n",
    "\t\tp = np.stack(_p)\n",
    "\t\tprint(X.shape, y.shape, p.shape)\n",
    "\n",
    "\t\treturn X, y, p\n",
    "\n",
    "\n",
    "\tdef get_signs(self, df):\n",
    "\t\tsigns = list(df.columns)\n",
    "\t\tsigns = [s for s in signs if s.endswith(\"_x\") or s.endswith(\"_y\") or s.endswith(\"_z\")]\n",
    "\t\texcluded_body_landmarks = [10, 11, 13, 14, 19, 20, 21, 22, 23, 24]\n",
    "\t\texcluded_body_landmarks = tuple([f\"pose_{i}\" for i in excluded_body_landmarks])\n",
    "\t\tunwanted_pose_columns = [i for i in list(signs) if i.startswith(excluded_body_landmarks)]\n",
    "\t\tsigns = [s for s in signs if s not in unwanted_pose_columns]\n",
    "\t\treturn signs\n",
    "\n",
    "\n",
    "\tdef prepare_data(self, df):\n",
    "\t\tif (self.dataset_name == \"minds\"):\n",
    "\t\t\tif \"person\" not in df.columns:\n",
    "\t\t\t\tdf[\"person\"] = df[\"video_name\"].apply(lambda i: int(re.findall(r\".*Sinalizador(\\d+)-.+.mp4\", i)[0]))\n",
    "\n",
    "\t\tcolumns = [\"category\", \"video_name\", \"person\", \"frame\"] + self.signs\n",
    "\t\tdf = df[columns]\n",
    "\t\tvideos = df[\"video_name\"].unique()\n",
    "\t\tdata = []\n",
    "\n",
    "\t\tfor video in videos:\n",
    "\t\t\tdf_video = df[df[\"video_name\"] == video].sort_values(\"frame\")\n",
    "\t\t\tcategory = df_video.iloc[0][\"category\"]\n",
    "\t\t\tperson = df_video.iloc[0][\"person\"]\n",
    "\n",
    "\t\t\tdf_video = df_video.drop([\"category\", \"video_name\", \"frame\"], axis=1)\n",
    "\t\t\tx = self.get_axis_df(df_video, \"x\")\n",
    "\t\t\ty = self.get_axis_df(df_video, \"y\")\n",
    "\n",
    "\t\t\tx = x.T.to_numpy()\n",
    "\t\t\ty = y.T.to_numpy()\n",
    "\n",
    "\t\t\tx = self.normalize_axis(x)\n",
    "\t\t\ty = self.normalize_axis(y)\n",
    "\n",
    "\t\t\tdata.append({\n",
    "\t\t\t\t\"x\": x,\n",
    "\t\t\t\t\"y\": y,\n",
    "\t\t\t\t\"video_name\": video,\n",
    "\t\t\t\t\"category\": category,\n",
    "\t\t\t\t\"person\": person\n",
    "\t\t\t})\n",
    "\n",
    "\t\treturn pd.DataFrame.from_dict(data)\n",
    "\n",
    "\n",
    "\tdef get_axis_df(self, df, axis):\n",
    "\t\treturn df[[c for c in self.signs if c.endswith(axis)]]\n",
    "\n",
    "\n",
    "\tdef normalize_axis(self, axis):\n",
    "\t\taxis[axis < 0] = 0\n",
    "\t\taxis[axis > 1] = 1\n",
    "\t\treturn axis\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3040, 3, 224, 224) (3040,) (3040,)\n"
     ]
    }
   ],
   "source": [
    "data = Data(dataset_name=dataset_name, image_method=image_method, transform=transform)\n",
    "\n",
    "X, y, p = data.get_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class Resnet18Model(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes: int):\n",
    "        super(Resnet18Model, self).__init__()\n",
    "\n",
    "        self.model = models.resnet18(pretrained=True)\n",
    "\n",
    "        # self.model.flatten = nn.Flatten()\n",
    "\n",
    "        num_ftrs = self.model.fc.in_features\n",
    "\n",
    "        self.model.fc = nn.Sequential(\n",
    "            nn.BatchNorm1d(num_ftrs),\n",
    "            nn.Linear(num_ftrs, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward_once(self, x):\n",
    "        output = self.model(x)\n",
    "\n",
    "        # output = output.view(output.size()[0], -1) # batches x feat_dim\n",
    "        # output = self.model.flatten(output)\n",
    "        # output = self.model.fc(output)\n",
    "\n",
    "        # print(x.shape, output.shape)\n",
    "        # print(self.model.fc)\n",
    "\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        output1 = self.forward_once(input1)\n",
    "        output2 = self.forward_once(input2)\n",
    "        return output1, output2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniele/.local/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/daniele/.local/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "num_features = len(np.unique(y))\n",
    "print(num_features)\n",
    "\n",
    "model = Resnet18Model(num_classes=num_features)\n",
    "model.to(device)\n",
    "\n",
    "criterion = ContrastiveLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original X shape: (3040, 3, 224, 224), y shape: (3040,)\n",
      "X_train shape: (1824, 3, 224, 224), y_train shape: (1824,)\n",
      "X_val shape: (608, 3, 224, 224), y_val shape: (608,)\n",
      "X_test shape: (608, 3, 224, 224), y_test shape: (608,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "# Supondo que X e y sejam arrays NumPy contendo as imagens e os rótulos\n",
    "# Verifique a forma dos dados antes da divisão\n",
    "print(f\"Original X shape: {X.shape}, y shape: {y.shape}\")\n",
    "\n",
    "# Certifique-se de que os dados estão no formato correto\n",
    "# X = (X * 255).astype(np.uint8)  # Converta para uint8 se necessário\n",
    "\n",
    "# Definir as transformações\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.Grayscale(num_output_channels=3),  # Converte para 3 canais\n",
    "])\n",
    "\n",
    "# Dividir os dados em conjuntos de treinamento, validação e teste\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Verifique a forma dos dados após a divisão\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape}, y_val shape: {y_val.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss 5.3533 (Best Epoch -1 Best Accuracy -1.000000)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 13\u001b[0m\n\u001b[1;32m      7\u001b[0m train_evaluator \u001b[38;5;241m=\u001b[39m Evaluate(labels\u001b[38;5;241m=\u001b[39my_val, k_shot\u001b[38;5;241m=\u001b[39mk_shot, num_runs\u001b[38;5;241m=\u001b[39mnum_runs, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# for images, labels, _ in train_dataloader:\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#     print(images.shape)  # Deve ser [batch_size, 3, 224, 224]\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#     break\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m best_epoch, best_accuracy, best_loss_history, loss_history, accuracy_history \u001b[38;5;241m=\u001b[39m \u001b[43mTraining\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43mevaluator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_evaluator\u001b[49m\u001b[43m,\u001b[49m\u001b[43mk_shot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk_shot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_epoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Best Accuracy \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m Training\u001b[38;5;241m.\u001b[39mchart(epochs, loss_history, accuracy_history)\n",
      "File \u001b[0;32m~/Projects/sign-language-similarity/execution/trainining.py:32\u001b[0m, in \u001b[0;36mTraining.exec\u001b[0;34m(model, train_dataloader, val_dataloader, num_epochs, criterion, optimizer, device, evaluator, k_shot)\u001b[0m\n\u001b[1;32m     30\u001b[0m output1, output2 \u001b[38;5;241m=\u001b[39m model(image1, image2)\n\u001b[1;32m     31\u001b[0m loss_contrastive \u001b[38;5;241m=\u001b[39m criterion(output1, output2, label)\n\u001b[0;32m---> 32\u001b[0m \u001b[43mloss_contrastive\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     35\u001b[0m loss_value \u001b[38;5;241m=\u001b[39m loss_contrastive\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_dataset = TrainingData(X=X_train, y=y_train, transform=None)\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, num_workers=num_workers, batch_size=batch_size)\n",
    "eval_dataset = EvaluateData(X=X_val, y=y_val, transform=None)\n",
    "eval_dataloader = DataLoader(eval_dataset, shuffle=False, num_workers=num_workers, batch_size=batch_size)\n",
    "test_dataset = EvaluateData(X=X_test, y=y_test, transform=None)\n",
    "test_dataloader = DataLoader(test_dataset, shuffle=False, num_workers=num_workers, batch_size=batch_size)\n",
    "train_evaluator = Evaluate(labels=y_val, k_shot=k_shot, num_runs=num_runs, device=device)\n",
    "\n",
    "# for images, labels, _ in train_dataloader:\n",
    "#     print(images.shape)  # Deve ser [batch_size, 3, 224, 224]\n",
    "#     break\n",
    "\n",
    "best_epoch, best_accuracy, best_loss_history, loss_history, accuracy_history = Training.exec(model=model,train_dataloader=train_dataloader, val_dataloader=eval_dataloader,num_epochs=epochs,criterion=criterion,optimizer=optimizer,device=device,evaluator=train_evaluator,k_shot=k_shot)\n",
    "print(f\"Best Epoch {best_epoch} Best Accuracy {best_accuracy:3f}\")\n",
    "Training.chart(epochs, loss_history, accuracy_history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
